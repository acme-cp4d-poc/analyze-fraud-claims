{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Online Trading Customer Attrition Risk Prediction using SparkML\n\nThere are many users of online trading platforms and these companies would like to run analytics on and predict churn based on user activity on the platform. Since competition is rife, keeping customers happy so they do not move their investments elsewhere is key to maintaining profitability.\n\nIn this notebook, we will leverage IBM Cloud Private for Data to do the following:\n\n1. Ingest merged customer demographics and trading activity data\n2. Visualize the merged dataset to get a better understanding of the data and build hypotheses for prediction\n3. Leverage the SparkML library to build a classification model that predicts whether a customer has a propensity to churn\n4. Expose the SparkML classification model as a RESTful API endpoint for the end-to-end customer churn risk prediction and risk remediation application\n\n<a id=\"top\"></a>\n## Table of Contents\n\n1. [Load the customer demographics and trading activity data](#load_data)\n2. [Load libraries](#load_libraries)\n3. [Visualize the customer demographics and trading activity data](#visualize)\n4. [Prepare data for building SparkML classification model](#prepare_data)\n5. [Train classification model and test model performance](#build_model)\n6. [Save model to ML repository and expose it as REST API endpoint](#save_model)\n7. [Summary](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Quick set of instructions to work through the notebook\n\nIf you are new to Notebooks, here's a quick overview of how to work in this environment.\n\n1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load_data\"></a>\n## 1. Load the customer and trading activity data\n[Top](#top)\n\nData can be easily loaded within IBM Cloud Private for Data using point-and-click functionality. The following image illustrates how to load the data from a database. The data set can be located by its name and inserted into the notebook as a pandas DataFrame as shown below.\n\n![insert_spark_dataframe.png](https://raw.githubusercontent.com/IBM/icp4d-customer-churn-classifier/master/doc/source/images/insert_spark_dataframe.png)\n\nThe generated code comes up with a generic name and it is good practice to rename the dataframe to match the use case context."}, {"metadata": {}, "cell_type": "code", "source": "# Use the find data 10/01 icon and under your remote data set\n# use \"Insert to code\" and \"Insert pandas DataFrame\"\n# here.\n\nimport os, pandas as pd\n# Add asset from file system\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# After inserting the pandas DataFrame code above, change the following\n# df_data_# to match the variable used in the above code. df_churn_pd is used\n# later in the notebook.\ndf_churn_pd = df_data_#", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load_libraries\"></a>\n## 2. Load libraries\n[Top](#top)\n\nRunning the following cell will load all libraries needed to load, visualize, prepare the data and build ML models for our use case.\n\nNOTE: restart of KERNEL may be required."}, {"metadata": {}, "cell_type": "code", "source": "# The following libraries are required for this notebook and should already be installed in the \n# kernel. If not, install them here. The versions listed have been tested to work with this notebook.\n\n# NOTE: if you do have to perform one or more of these installs, you MUST restart the kernel\n\n#!pip install --user pyspark==2.3.2\n#!pip install --user py4j==0.10.7\n#!pip install --user watson-machine-learning-client==1.0.375\n\n# check if libraries are loaded\n#!pip freeze", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier, NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport brunel\n\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pyspark.sql import SparkSession\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"visualize\"></a>\n## 3. Visualize the customer demographics and trading activity data\n[Top](#top)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Data visualization is a key step in the data mining process that helps to better understand the data before it can be prepared for building ML models.\n\nWe will use the Brunel visualization which comes preloaded in IBM Cloud Private for Data analytics projects. \n\nThe Brunel Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and business users. More information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Churn risk count"}, {"metadata": {}, "cell_type": "code", "source": "%brunel data('df_churn_pd') stack polar bar x(CHURNRISK) y(#count) color(CHURNRISK) bar tooltip(#all) :: width=300, height=300", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Marital status count and churn risk percentage"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "%brunel data('df_churn_pd') bar x(STATUS) y(#count) color(STATUS) tooltip(#all) | stack bar x(STATUS) y(#count) color(CHURNRISK: pink-orange-yellow) bin(STATUS) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Churn risk count by total units traded"}, {"metadata": {}, "cell_type": "code", "source": "%brunel data('df_churn_pd') stack bar x(TOTALUNITSTRADED:[0, 350]) y(#count) color(CHURNRISK: pink-gray-orange) sort(STATUS) label(#count) tooltip(#all) :: width=1200, height=350 \n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Churn risk percentage by days since last trade"}, {"metadata": {}, "cell_type": "code", "source": "%brunel data('df_churn_pd') stack bar x(DAYSSINCELASTTRADE) y(#count) color(CHURNRISK: pink-gray-orange) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 \n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"prepare_data\"></a>\n## 4. Data preparation\n[Top](#top)\n\nData preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n\nDuring this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices and encoded using One-hot encoding to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n\nFinal step in the data preparation process is to assemble all the categorical and non-categorical columns into a feature vector. We use VectorAssembler for this. VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models."}, {"metadata": {}, "cell_type": "code", "source": "# Defining the categorical columns \ncategoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "non_categoricalColumns = [c for c in df_churn_pd.columns if c not in categoricalColumns]\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(non_categoricalColumns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "non_categoricalColumns.remove('CHURNRISK')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Create a Spark session\nspark = SparkSession.builder.getOrCreate()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "stages = []\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    \n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    encoder = OneHotEncoder(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n    \n    stages += [stringIndexer, encoder]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "spark_df_churn = spark.createDataFrame(df_churn_pd)\nlabelIndexer = StringIndexer(inputCol='CHURNRISK', outputCol='label').fit(spark_df_churn)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for colnum in non_categoricalColumns:\n    spark_df_churn = spark_df_churn.withColumn(colnum, spark_df_churn[colnum].cast(IntegerType()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Transform all features into a vector using VectorAssembler\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + non_categoricalColumns\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"build_model\"></a>\n## 5. Build SparkML Random Forest classification model\n[Top](#top)"}, {"metadata": {}, "cell_type": "markdown", "source": "We instantiate a decision-tree based classification algorithm, namely, RandomForestClassifier. Next we define a pipeline to chain together the various transformers and estimaters defined during the data preparation step before. MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n\nWe split original dataset into train and test datasets. We fit the pipeline to training data and apply the trained model to transform test data and generate churn risk class prediction"}, {"metadata": {}, "cell_type": "code", "source": "# Instantiate a random forest classifier, take the default settings\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n\nstages += [labelIndexer, assembler, rf, labelConverter]\n\npipeline = Pipeline(stages=stages)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Split data into train and test datasets\ntrain, test = spark_df_churn.randomSplit([0.7,0.3], seed=100)\ntrain.cache()\ntest.cache()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Build models\nmodel = pipeline.fit(train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "results = model.transform(test)\nresults = results.select(results[\"ID\"],results[\"CHURNRISK\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\nresults.toPandas().head(6)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Model results\n\nIn a supervised classification problem such as churn risk classification, we have a true output and a model-generated predicted output for each data point. For this reason, the results for each data point can be assigned to one of four categories:\n\n1. True Positive (TP) - label is positive and prediction is also positive\n2. True Negative (TN) - label is negative and prediction is also negative\n3. False Positive (FP) - label is negative but prediction is positive\n4. False Negative (FN) - label is positive but prediction is negative\n\nThese four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering classifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The reason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from a dataset where 95% of the data points are not fraud and 5% of the data points are fraud, then a naive classifier that predicts not fraud, regardless of input, will be 95% accurate. For this reason, metrics like precision and recall are typically used because they take into account the type of error. In most applications there is some desired balance between precision and recall, which can be captured by combining the two into a single metric, called the F-measure.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "print('Model Precision = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count())))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "An added advantage of such tree-based classifiers is we can study feature importances and learn further about relative importances of features in the classification decision."}, {"metadata": {}, "cell_type": "code", "source": "# Evaluate model\n\n# Compute raw scores on the test set\nres = model.transform(test)\npredictions = res.rdd.map(lambda pr: pr.prediction)\nlabels = res.rdd.map(lambda pr: pr.label)\n# predictionAndLabels = spark.parallelize(zip(predictions.collect(), labels.collect()))\npredictionAndLabels = spark.sparkContext.parallelize(zip(predictions.collect(), labels.collect()))\n\n# Instantiate metrics object\nmetrics = MulticlassMetrics(predictionAndLabels)\n\n# Overall statistics\nprint(\"Overall Statistics\")\nf_measure = metrics.accuracy\nprint(\"Model F-measure = %s\\n\" % f_measure)\n\n# statistics by class\nprint(\"Statistics by Class\")\nlabels_itr = labels.distinct().collect()\nfor label in sorted(labels_itr):\n    print(\"Class %s F-Measure = %s\" % (label, metrics.fMeasure(label)))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Feature importance\n\nrfModel = model.stages[-2]\n\nfeatures = df_churn_pd.columns\nimportances = rfModel.featureImportances.values\nindices = np.argsort(importances)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.figure(1)\nplt.title('Feature Importance')\nplt.barh(range(len(indices)), importances[indices], color='b',align='center')\nplt.yticks(range(len(indices)), (np.array(features))[indices])\nplt.xlabel('Relative Importance')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Before we save the random forest classifier to repository, let us first evaluate the performance of a simple Naive Bayes classifier trained on the training dataset. "}, {"metadata": {}, "cell_type": "code", "source": "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n\nstages_nb = stages\n\nstages_nb[-2] = nb\n\npipeline_nb = Pipeline(stages = stages_nb)\n\n# Build models\nmodel_nb = pipeline_nb.fit(train)\nresults_nb = model_nb.transform(test)\n\nprint('Naive Bayes Model Precision = {:.2f}.'.format(results_nb.filter(results_nb.label == results_nb.prediction).count() / float(results_nb.count())))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see from the results above, Naive Bayes classifier does not perform well. Random forest classifier shows high F-measure upon evaluation and shows strong performance. Hence, we will save this model to the repository."}, {"metadata": {}, "cell_type": "markdown", "source": "### Save and deploy the model using Watson Machine Learning\n\nThe Watsom Machine Learning client should be available on your IBM Cloud Pak for Data platform. "}, {"metadata": {}, "cell_type": "code", "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n\n# get URL, username and password from your IBM Cloud Pak for Data administrator\nwml_credentials = {\n  \"url\": \"https://X.X.X.X\",\n  \"instance_id\": \"icp\",\n  \"username\": \"****\",\n  \"password\": \"****\"\n}\n\nclient = WatsonMachineLearningAPIClient(wml_credentials)\nprint(client.version)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Use this cell to do any cleanup of previously created models and deployments\nclient.repository.list_models()\nclient.deployments.list()\n\n# client.repository.delete('GUID of stored model')\n# client.deployments.delete('GUID of deployed model')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Store our model\nmodel_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"IBM\", \n               client.repository.ModelMetaNames.NAME: \"Trading Customer Churn Prediction Model\"}\npublished_model = client.repository.store_model(model=model, pipeline=pipeline, meta_props=model_props, training_data=train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# new list of models\nclient.repository.list_models()\n\n# get UID of our just stored model\nmodel_uid = client.repository.get_model_uid(published_model)\nprint(\"Model id: {}\".format(model_uid))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "created_deployment = client.deployments.create(model_uid , name=\"Trading Customer Churn Deployment\")\n\n# new list of deployments\nclient.deployments.list()\n\n# get UID of our new deployment\ndeployment_uid = client.deployments.get_uid(created_deployment)\nprint(\"Deployment id: {}\".format(deployment_uid))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(created_deployment)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# get the scoring endpoint for the deployed WML model\nscoring_endpoint = client.deployments.get_scoring_url(created_deployment)\nprint(scoring_endpoint)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# test the model using array of values\nimport json\nvalues = [4,\"F\",\"M\",2,52004,\"N\",60,5030,23,1257,125,3,1,1,1000,0]\nfields = [\"ID\", \"GENDER\", \"STATUS\",\"CHILDREN\",\"ESTINCOME\",\"HOMEOWNER\",\"AGE\",\"TOTALDOLLARVALUETRADED\",\"TOTALUNITSTRADED\",\"LARGESTSINGLETRANSACTION\",\"SMALLESTSINGLETRANSACTION\",\"PERCENTCHANGECALCULATION\",\"DAYSSINCELASTLOGIN\",\"DAYSSINCELASTTRADE\",\"NETREALIZEDGAINS_YTD\",\"NETREALIZEDLOSSES_YTD\"]\nscoring_payload = {\"fields\": fields, \"values\": [values]}\nprint(json.dumps(scoring_payload, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# score the model by calling the WML service with the user provided data\npredictions = client.deployments.score(scoring_endpoint, scoring_payload)\n# print(json.dumps(predictions, indent=2))\nprint(predictions)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Save and deploy the model locally so it will be added as an asset to your IBM Cloud Pak for Data project"}, {"metadata": {}, "cell_type": "code", "source": "from dsx_ml.ml import save\nsave(name='TradingChurnRiskClassificationSparkML',\n    model=model,\n    test_data = test,\n    algorithm_type='Classification',\n    description='This is a SparkML Model to Classify Trading Customer Churn Risk')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Write the test data without label to a .csv so that we can later use it for batch scoring\nwrite_score_CSV=test.toPandas().drop(['CHURNRISK'], axis=1)\nwrite_score_CSV.to_csv('../datasets/TradingCustomerSparkMLBatchScore.csv', sep=',', index=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Write the test data to a .csv so that we can later use it for evaluation\nwrite_eval_CSV=test.toPandas()\nwrite_eval_CSV.to_csv('../datasets/TradingCustomerSparkMLEval.csv', sep=',', index=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<p><font size=-1 color=gray>\n&copy; Copyright 2018 IBM Corp. All Rights Reserved.\n<p>\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\nexcept in compliance with the License. You may obtain a copy of the License at\nhttps://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\nexpress or implied. See the License for the specific language governing permissions and\nlimitations under the License.\n</font></p>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}